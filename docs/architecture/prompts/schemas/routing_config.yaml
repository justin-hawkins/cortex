# Routing Configuration for Distributed Agentic Task System
# Defines model endpoints and tier assignments
#
# Server Reference (see servers.yaml for full details):
#   - 192.168.1.11: AMD Epyc 32-core (Ollama CPU mode + vLLM with 2x RTX5060Ti)
#   - 192.168.1.12: 2x RTX4060Ti 16GB (Ollama GPU mode)

model_tiers:
  tiny:
    context_window: 32000
    safe_working_limit: 22000
    models:
      - name: gemma3:4b
        endpoint: "http://192.168.1.12:11434"  # rtx4060_server
        type: ollama
      - name: qwen3-4b-m32k:latest
        endpoint: "http://192.168.1.12:11434"  # rtx4060_server
        type: ollama
        priority: 2

  small:
    context_window: 32000
    safe_working_limit: 22000
    models:
      - name: gemma3:12b
        endpoint: "http://192.168.1.12:11434"  # rtx4060_server
        type: ollama
      - name: gemma3:27b
        endpoint: "http://192.168.1.12:11434"  # rtx4060_server
        type: ollama
        priority: 2

  large:
    context_window: 64000
    safe_working_limit: 45000
    models:
      - name: openai/gpt-oss-20b
        endpoint: "http://192.168.1.11:8000/v1"  # epyc_server vLLM (GPU)
        type: openai_compatible
        priority: 1  # Use first for routine tasks
      - name: qwen3-coder:30b-a3b-q8_0-64k
        endpoint: "http://192.168.1.11:11434"  # epyc_server Ollama (CPU)
        type: ollama
        priority: 2  # Use for complex coding tasks (faster)
      - name: qwen3-coder:30b-a3b-fp16-64k
        endpoint: "http://192.168.1.11:11434"  # epyc_server Ollama (CPU)
        type: ollama
        priority: 3  # Use for complex coding tasks (higher precision)

  frontier:
    context_window: 200000
    safe_working_limit: 140000
    models:
      - name: claude-sonnet-4-20250514
        endpoint: "https://api.anthropic.com/v1"
        type: anthropic

embedding:
  model: mxbai-embed-large:335m
  endpoint: "http://192.168.1.12:11434"  # rtx4060_server
  type: ollama

# Agent role to model assignments
agent_routing:
  coordinator:
    preferred_tier: large
    preferred_model: qwen3-coder:30b-a3b-q8_0-64k
    fallback_tier: frontier

  decomposer:
    preferred_tier: large
    preferred_model: qwen3-coder:30b-a3b-q8_0-64k
    fallback_tier: frontier

  complexity_estimator:
    preferred_tier: large
    preferred_model: gpt-oss:20b

  qa_reviewer:
    preferred_tier: large
    preferred_model: qwen3-coder:30b-a3b-q8_0-64k
    min_tier_vs_worker: same_or_higher

  qa_reviewer_adversarial:
    preferred_tier: large
    preferred_model: qwen3-coder:30b-a3b-fp16-64k
    fallback_tier: frontier

  merge_coordinator:
    preferred_tier: large
    preferred_model: qwen3-coder:30b-a3b-q8_0-64k
    fallback_tier: frontier
