"""
Mock model client for testing DATS.

Provides predictable responses without requiring external model servers.
"""

import asyncio
import json
import re
from dataclasses import dataclass
from typing import Any, Optional

from src.models.base import BaseModelClient, ModelResponse


@dataclass
class MockResponse:
    """Predefined response for mock client."""

    pattern: str  # Regex pattern to match against prompt
    response: str  # Response to return
    tokens_input: int = 100
    tokens_output: int = 50


# Default mock responses for different task types
DEFAULT_MOCK_RESPONSES = [
    # Fibonacci function
    MockResponse(
        pattern=r"fibonacci|fib\s*number",
        response="""```python
def fibonacci(n: int) -> int:
    \"\"\"
    Calculate the nth Fibonacci number.
    
    Args:
        n: The position in the Fibonacci sequence (0-indexed)
        
    Returns:
        The nth Fibonacci number
        
    Examples:
        >>> fibonacci(0)
        0
        >>> fibonacci(1)
        1
        >>> fibonacci(10)
        55
    \"\"\"
    if n < 0:
        raise ValueError("n must be non-negative")
    if n <= 1:
        return n
    
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b
```""",
        tokens_input=150,
        tokens_output=200,
    ),
    # Coordinator analysis
    MockResponse(
        pattern=r"coordinator|analyze|orchestrat",
        response="""{
    "mode": "new_project",
    "domain": "code-general",
    "needs_decomposition": false,
    "complexity": "small",
    "acceptance_criteria": "Function should be correct and well-documented"
}""",
        tokens_input=100,
        tokens_output=80,
    ),
    # Decomposer response
    MockResponse(
        pattern=r"decompos|break.*down|subtask",
        response="""{
    "subtasks": [
        {
            "id": "subtask-1",
            "description": "Implement core logic",
            "domain": "code-general",
            "complexity": "small",
            "is_atomic": true
        }
    ],
    "dependencies": []
}""",
        tokens_input=120,
        tokens_output=100,
    ),
    # Complexity estimation
    MockResponse(
        pattern=r"complex|estimat|tier|route",
        response="""{
    "recommended_tier": "small",
    "estimated_tokens": 2000,
    "confidence": 0.85,
    "reasoning": "Simple algorithmic task with clear requirements",
    "required_capabilities": ["python"],
    "qa_profile": "consensus"
}""",
        tokens_input=100,
        tokens_output=90,
    ),
    # QA review
    MockResponse(
        pattern=r"review|qa|quality|validat",
        response="""{
    "status": "approved",
    "score": 0.9,
    "issues": [],
    "feedback": "Code is well-structured and documented"
}""",
        tokens_input=150,
        tokens_output=60,
    ),
    # General code generation
    MockResponse(
        pattern=r"function|class|implement|code|create|write",
        response="""```python
def example_function():
    \"\"\"Example function generated by mock client.\"\"\"
    return "Hello, World!"
```""",
        tokens_input=100,
        tokens_output=50,
    ),
    # Default fallback
    MockResponse(
        pattern=r".*",
        response="Task completed successfully.",
        tokens_input=50,
        tokens_output=20,
    ),
]


class MockModelClient(BaseModelClient):
    """
    Mock model client for testing.
    
    Provides predictable responses based on pattern matching,
    allowing tests to run without external model servers.
    """

    def __init__(
        self,
        endpoint: str = "mock://localhost",
        model_name: str = "mock-model",
        responses: Optional[list[MockResponse]] = None,
        latency_ms: int = 0,
    ):
        """
        Initialize the mock client.
        
        Args:
            endpoint: Fake endpoint (not used)
            model_name: Model name for responses
            responses: Custom response patterns (uses defaults if None)
            latency_ms: Simulated latency in milliseconds
        """
        super().__init__(endpoint, model_name)
        self._responses = responses or DEFAULT_MOCK_RESPONSES
        self._latency_ms = latency_ms
        self._call_history: list[dict[str, Any]] = []

    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        stop_sequences: Optional[list[str]] = None,
    ) -> ModelResponse:
        """
        Generate a mock response based on prompt patterns.
        
        Args:
            prompt: The user prompt
            system_prompt: Optional system prompt
            temperature: Sampling temperature (ignored)
            max_tokens: Maximum tokens (ignored)
            stop_sequences: Stop sequences (ignored)
            
        Returns:
            ModelResponse with matched pattern response
        """
        # Record the call
        self._call_history.append({
            "prompt": prompt,
            "system_prompt": system_prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
        })

        # Simulate latency
        if self._latency_ms > 0:
            await asyncio.sleep(self._latency_ms / 1000)

        # Find matching response
        combined_text = f"{system_prompt or ''} {prompt}".lower()
        
        for mock_response in self._responses:
            if re.search(mock_response.pattern, combined_text, re.IGNORECASE):
                return ModelResponse(
                    content=mock_response.response,
                    tokens_input=mock_response.tokens_input,
                    tokens_output=mock_response.tokens_output,
                    model=self.model_name,
                    finish_reason="stop",
                    raw_response={"mock": True, "pattern": mock_response.pattern},
                )

        # Fallback
        return ModelResponse(
            content="Mock response: Task acknowledged.",
            tokens_input=50,
            tokens_output=20,
            model=self.model_name,
            finish_reason="stop",
            raw_response={"mock": True, "pattern": "fallback"},
        )

    async def generate_stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        stop_sequences: Optional[list[str]] = None,
    ):
        """
        Generate a streaming mock response.
        
        Yields the full response in chunks.
        """
        response = await self.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            stop_sequences=stop_sequences,
        )

        # Simulate streaming by yielding words
        words = response.content.split()
        for i, word in enumerate(words):
            if self._latency_ms > 0:
                await asyncio.sleep(self._latency_ms / 1000 / len(words))
            yield word + (" " if i < len(words) - 1 else "")

    def count_tokens(self, text: str) -> int:
        """
        Estimate token count.
        
        Uses simple word-based estimation (1 word â‰ˆ 1.3 tokens).
        """
        return int(len(text.split()) * 1.3)

    def get_call_history(self) -> list[dict[str, Any]]:
        """Get history of all calls made to this client."""
        return self._call_history.copy()

    def clear_call_history(self):
        """Clear the call history."""
        self._call_history.clear()

    def add_response(
        self,
        pattern: str,
        response: str,
        tokens_input: int = 100,
        tokens_output: int = 50,
    ):
        """
        Add a custom response pattern.
        
        Args:
            pattern: Regex pattern to match
            response: Response to return
            tokens_input: Input token count
            tokens_output: Output token count
        """
        # Insert at beginning to take priority
        self._responses.insert(0, MockResponse(
            pattern=pattern,
            response=response,
            tokens_input=tokens_input,
            tokens_output=tokens_output,
        ))

    def set_latency(self, latency_ms: int):
        """Set simulated latency in milliseconds."""
        self._latency_ms = latency_ms


def create_mock_client_for_tier(tier: str) -> MockModelClient:
    """
    Create a mock client configured for a specific tier.
    
    Args:
        tier: Model tier (tiny, small, large, frontier)
        
    Returns:
        Configured MockModelClient
    """
    tier_models = {
        "tiny": "mock-gemma-4b",
        "small": "mock-gemma-12b",
        "large": "mock-qwen-coder",
        "frontier": "mock-claude-sonnet",
    }
    
    return MockModelClient(
        model_name=tier_models.get(tier, "mock-model"),
    )


# Global mock client registry for testing
_mock_clients: dict[str, MockModelClient] = {}


def register_mock_client(name: str, client: MockModelClient):
    """Register a mock client for testing."""
    _mock_clients[name] = client


def get_mock_client(name: str) -> Optional[MockModelClient]:
    """Get a registered mock client."""
    return _mock_clients.get(name)


def clear_mock_clients():
    """Clear all registered mock clients."""
    _mock_clients.clear()